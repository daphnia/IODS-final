---
title: "Final Assignment"
author: "Hans Hellén, hans.hellen@hellen.me"
date: "Dec 18 2017"
output: 
  html_document:
    theme: journal
    toc: true
    toc_depth: 2
    fig_caption: true
    fig_width: 10
    fig_height: 8
    code_folding: hide
---

<style type="text/css">

body{ /* Normal  */
      font-size: 20px;
  }
</style>



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning =FALSE, message = FALSE)
```

```{r echo=T}
library(MASS)
library(dplyr)
library(ggplot2)
library(knitr)
library(tidyr)
library(GGally)
boston_final <- read.csv("data/boston_final.csv", header=T, sep=",")

#NOTE: some functions might have be commented out to make the knitted output more simple.
```


<!-- 
#INSTRUCTIONS:
Make a clear structure

Brief description of the "research question" you are exploring, possibly including your hypothesis (max 2 points)

A link to your data wrangling script. See the general instructions. (max 5 points)

Description of your data and its variables. Where is the data from, what does it relate to, what do the variables represent, what has been done to the data before analysis? (max 2 points)

Visually clear and interesting explorations of the variables of interest in the data, from the point of view of your research question. Include interpretations of the distributions and relationships of the variables. Use captions to draw the reader’s focus on the interesting parts of your tables and graphics. (max 8 points)

Brief description of the method(s) you are using in your own words (max 3 points)

Presentation of the results of your analysis including visualizations and summaries and a thorough interpretation of the results including a validation analysis of the method.(max 16 points)

Conclusions and discussion (max 2 points)

An ‘abstract’ at the beginning of the page with a summary of your analysis (max 2 points)

The total maximum of Final Assignment is 40 points.

POLISHing, optional:
Look at the RMarkdown html document options and finalize the appearance of your page: http://rmarkdown.rstudio.com/html_document_format.html.

-->


#Introduction

Originally the data I will use is from Boston dataset from R package/library called MASS. This data frame is about housing values in suburbs of Boston. The original variables are explained on [this](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) webpage. 

In short the variables are about crime rate per capita, urban planning, housing, air pollution, economy, school environment, skin colour. It is useful to have look at the original [research article](https://dx.doi.org/10.1016/0095-0696%2878%2990006-2), that was about air pollution, the housing values (variable medv) and willingness to pay for cleaner air. 

I will fit a linear regression model to find out **what variables explain best the highest criminal activity occurring in Boston**. According the knowledge gained from LDA biplot done in RStudio exercise 4, my hypothesis is that the explanatory variables *rad, lstat, tax, ptratio, black* and *rm* can be used to best predict high crime rate. 



#Data Wrangling
[Here](https://github.com/daphnia/IODS-final/blob/master/data/create_final.R) is my data wrangling script. 

As can clearly be seen from a histogram of the original data below, there is a small bunch of suburbs with extreme crime rates per capita. 

```{r echo=T, fig.cap="Histogram from original data concerning Boston"}
data("Boston")
p_crim <- ggplot(Boston, aes(crim)) + geom_histogram(binwidth = 1, fill="purple", colour="red") + ggtitle("Histogram of per capita crime rate by town")
p_crim
```

In the following analysis the focus is on the highest quantile of crime rates. 

#Analysis

Here is summary of my data used here:

```{r echo=T}
glimpse(boston_final)
summary(boston_final)
```
Scatter plots of all the variables are presented in a matrix below. 
```{r echo=T, fig.height=18, fig.width=18, fig.cap="**Tip**: Right click the image of plots and choose \"Show image\" to zoom closer."}
p_b_f <- ggpairs(boston_final, lower = list(combo = wrap("facethist", bins = 20)))
p_b_f
```

The date set has observations from 127 different towns. The target (also known as dependent, response) variable is chosen to be crim (per capita crime rate by town). Keep in mind that the variable black gets smaller values as the proportion of black population increases.

From the summaries and graphics above we can already see some interesting findings concerning the high crime towns:

 * The *Zn* variable (proportion of residential land zoned for lots over 25,000 sq.ft.) has zero observations. We can conclude that in areas with big apartment houses, the high crime rate is improbable. So Zn will be left out from the modelling. 
 * Also there seems to be a separation into communities for black people and communities for white people. This can be seen from the scatter plot graphics: the density curve of *black* has two peaks.
 * Also looking at the scatter plot graphics and the the density curve of *medv* with two peaks. There seems to be a small amount of towns, where the median value of owner-occupied homes are exceptionally high. 
 * Most of the houses are old (*age*). 
 * The variable *indus*, *rad*, *tax* and *ptratio* has only one observation differing from all the others, so they are not suitable for linear regression analysis. So, my hypothesis just got weakened a lot. 

##Regression models

Here is regression models with multiple explanatory variables:
```{r echo=T}
my_model <- lm(crim ~ . - zn - indus - rad - tax - ptratio, data = boston_final)
summary(my_model)
```
Second try: 
```{r echo=T}
my_model2 <- lm(crim ~ . - zn - indus - rad - tax - ptratio - age - rm - black - lstat, data = boston_final)
summary(my_model2)
```
From the summary above it can be seen that 

 * when living far away from five Boston employment centres (*dis*), it predicts lower crime rate, 
 * when the values of homes (*medv*) increases, it predicts lower crime rate, 
 * when content of nitrous oxides (*nox*) increases, it predicts lower crime rate. 
 
Now, after looking the scatter plots again, the nox has to be left out, because the distribution is parabolic. So, only distance and housing value variables are left to my best model and have statistically significant relationship with high crime rate: 
```{r echo=T}
my_best_model <- lm(crim ~ dis + medv, data = boston_final)
summary(my_best_model)
```

Standard errors of regression coefficient for *dis* (1.8839) and for *medv* (0.1192), are not an order of magnitude less than the coefficients themselves (-8.6477 and -0.4811, respectively), indicating that there is some variability in the estimates for the coefficients. 

R-squared is about 24 %. It means that on the average only 24 % of variability in predicted crime rate can be explained by these two variables. The F-statistic indicates, would the model be better with fewer variables. In that case the p-value would be higher. Now p-value is 2.966-08 as it was in the beginning 3.864e-07. Although, this last model is better than with *nox* variable (p-value 8.182e-09).


##Model validation

Let's make some graphical model validation using the following plots: Residuals vs Fitted values, Normal QQ-plot and Residuals vs Leverage.

```{r echo=T, fig.cap="**Validation plots for a model called *my_best_model* explaining high crime rate with values of homes (*medv*) and content of nitrous oxides (*nox*)**"}
par(mfrow = c(2,2))
plot(my_best_model, which = c(1,2,5))
```

All the plots indicates that there are problems with this model. 

1. Residuals vs fitted values is a plot that can be used to check the model assumption that the residuals (ε, size of the errors) do not depend on explanatory variables but only on random variability (in other words, residuals has a constant variance <!--$σ^{2}$-->. In the plot the dots should be randomly distributed, otherwise it indicates a problem.
    i) In this case there can be seen a clear pattern on the right side of the picture.
2. Normal QQ-plot can be used to check, are residuals distributed normally. If they are, the dots should be aligned on the line.
    i) In these cases there are about ten outliers.
3. Residuals vs leverage is a plot that can be used to assess how big effect a single observation has to the model. In the plot a big x value means exceptionally big effect and refers to that the observation in question might be an outlier (*Note*: if an exceptional value (y axle) of target variable is near the mean of explanatory variables of the model, the observation does not have so big (leverage) effect to the regression line).
    i) There is clearly about five exceptional values with strong leverage and also about five other exceptional observations.


#Conclusions and discussion
The model fitted could not pass the graphical model validation, so this exercise should be done again with refined data and methods. Still the model seems to be realistic, as it predicts extreme crime rates are more probable nearer the densely populated employment centres (*dis*) and where the values of homes (*medv*) are lower. Removing some outliers the distributions in scatter plots would become more linear (now approaching asymptotically the axes). Unfortunately this regression analysis could not find any more socially interesting predictors among high crime rate towns. 
For example, from the original Boston dataset it could be seen that towns with the most highest crime rates had also the highest or second highest values of *tax* variable. Excerpt from the original article mentioned in the introduction of this work Harrison and Rubinfeld (1978) describes the *tax* variable: 

> Full value property tax rate ($/$1O,OOO), measures the cost of public services in each community. Nominal tax rates were corrected by local assessment ratios to yield the full value tax rate for each town. The coefficient of this variable should be negative. 

The last statement means that the lower the *tax* the higher is the housing values (*medv*) to be expected in the area. Crime obviously increases the cost of public services, so crim and tax are correlated. From the more succesful LDA analysis in RStudio exercise 4 *tax* seemed to classify medium–high crime areas but not at all high crime areas.  

***
The last time this document was knitted: `r date()`.